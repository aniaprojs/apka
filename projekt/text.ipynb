{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ania/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1019\n",
      "['As', 'I', 'stepped', 'through', 'the', 'door', 'of', 'the', 'ramen', 'restaurant', ',', 'a', 'wave', 'of', 'savory', 'aromas', 'enveloped', 'me', ',', 'instantly', 'tantalizing', 'my', 'taste', 'buds', 'and', 'stirring', 'memories', 'of', 'past', 'culinary', 'adventures', '.', 'The', 'ambiance', 'was', 'cozy', 'yet', 'bustling', ',', 'with', 'diners', 'nestled', 'into', 'every', 'available', 'nook', ',', 'their', 'chopsticks', 'dancing', 'eagerly', 'over', 'steaming', 'bowls', 'of', 'noodles', '.', 'I', 'found', 'myself', 'drawn', 'to', 'a', 'seat', 'at', 'the', 'counter', ',', 'where', 'I', 'could', 'watch', 'the', 'masterful', 'chefs', 'at', 'work', ',', 'their', 'hands', 'a', 'blur', 'of', 'motion', 'as', 'they', 'expertly', 'crafted', 'each', 'bowl', 'with', 'precision', 'and', 'care', '.', 'The', 'menu', 'offered', 'a', 'dizzying', 'array', 'of', 'options', ',', 'from', 'classic', 'tonkotsu', 'to', 'innovative', 'fusion', 'creations', ',', 'each', 'promising', 'a', 'journey', 'through', 'the', 'rich', 'tapestry', 'of', 'Japanese', 'flavors', '.', 'After', 'much', 'deliberation', ',', 'I', 'settled', 'on', 'the', 'house', 'special', ':', 'a', 'hearty', 'miso', 'ramen', 'adorned', 'with', 'slices', 'of', 'tender', 'chashu', 'pork', ',', 'perfectly', 'soft-boiled', 'eggs', ',', 'and', 'an', 'array', 'of', 'vibrant', 'vegetables', '.', 'As', 'I', 'waited', 'for', 'my', 'order', ',', 'I', 'sipped', 'on', 'a', 'cup', 'of', 'fragrant', 'green', 'tea', ',', 'letting', 'its', 'warmth', 'soothe', 'my', 'soul', 'and', 'prepare', 'my', 'palate', 'for', 'the', 'feast', 'to', 'come', '.']\n",
      "['array', 'door', 'savory', 'past'] After much deliberation, I settled on the house special: a hearty miso ramen adorned with slices of tender chashu pork, perfectly soft-boiled eggs, and an array of vibrant vegetables.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "import nltk\n",
    "from collections import Counter\n",
    "import spacy\n",
    "nltk.download('punkt')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"\"\"\n",
    "As I stepped through the door of the ramen restaurant, a wave of savory aromas enveloped me, instantly tantalizing my taste buds and stirring memories of past culinary adventures. The ambiance was cozy yet bustling, with diners nestled into every available nook, their chopsticks dancing eagerly over steaming bowls of noodles.\n",
    "\n",
    "I found myself drawn to a seat at the counter, where I could watch the masterful chefs at work, their hands a blur of motion as they expertly crafted each bowl with precision and care. The menu offered a dizzying array of options, from classic tonkotsu to innovative fusion creations, each promising a journey through the rich tapestry of Japanese flavors.\n",
    "\n",
    "After much deliberation, I settled on the house special: a hearty miso ramen adorned with slices of tender chashu pork, perfectly soft-boiled eggs, and an array of vibrant vegetables. As I waited for my order, I sipped on a cup of fragrant green tea, letting its warmth soothe my soul and prepare my palate for the feast to come.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_hashtags(text):\n",
    "    doc = nlp(text)    \n",
    "    nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "    adjectives = [token.text for token in doc if token.pos_ == \"ADJ\"]\n",
    "    noun_counts = Counter(nouns)\n",
    "    adjective_counts = Counter(adjectives)\n",
    "    top_nouns = noun_counts.most_common(2)\n",
    "    top_adjectives = adjective_counts.most_common(2)\n",
    "    hashtags = [noun[0] for noun in top_nouns] + [adj[0] for adj in top_adjectives]\n",
    "    return hashtags\n",
    "\n",
    "def get_summary(text):\n",
    "    stopWords = set(stopwords.words(\"english\")) \n",
    "    words = word_tokenize(text)\n",
    "    print(words)\n",
    "    # Creating a frequency table to keep the  score of each word    \n",
    "    freqTable = dict() \n",
    "    for word in words: \n",
    "        word = word.lower() \n",
    "        if word in stopWords: \n",
    "            continue\n",
    "        if word in freqTable: \n",
    "            freqTable[word] += 1\n",
    "        else: \n",
    "            freqTable[word] = 1\n",
    "\n",
    "    # Creating a dictionary to keep the score of each sentence \n",
    "    sentences = sent_tokenize(text) \n",
    "    sentenceValue = dict() \n",
    "    \n",
    "    for sentence in sentences: \n",
    "        for word, freq in freqTable.items(): \n",
    "            if word in sentence.lower(): \n",
    "                if sentence in sentenceValue: \n",
    "                    sentenceValue[sentence] += freq\n",
    "                else: \n",
    "                    sentenceValue[sentence] = freq \n",
    "    \n",
    "    sumValues = 0\n",
    "    for sentence in sentenceValue: \n",
    "        sumValues += sentenceValue[sentence] \n",
    "    \n",
    "    # Highest value sentence will be the summary. \n",
    "    summary = max(sentenceValue, key=sentenceValue.get)\n",
    "    return summary\n",
    "\n",
    "print(len(text))\n",
    "hashtags = get_hashtags(text)\n",
    "summary = get_summary(text)\n",
    "print(hashtags, summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
